{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "caff0a1b-3ecb-405a-872c-07ae3040b571",
   "metadata": {
    "is_executing": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'psycopg2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SparkSession\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpsycopg2\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdatetime\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m date\n\u001b[1;32m      5\u001b[0m today \u001b[38;5;241m=\u001b[39m date\u001b[38;5;241m.\u001b[39mtoday()\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mb-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'psycopg2'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "import psycopg2\n",
    "from datetime import date\n",
    "today = date.today().strftime(\"%b-%d-%Y\")\n",
    "spark = SparkSession.builder \\\n",
    "    .appName('Postgres to S3 pipeline') \\\n",
    "    .config('spark.jars','/opt/spark/jars/aws-java-sdk-bundle-1.11.375.jar')\\\n",
    "    .config('spark.jars','/opt/spark/jars/postgresql-42.3.5.jar')\\\n",
    "    .getOrCreate()\n",
    "# spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c810b0e-d8d0-486d-83f4-150febe882cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jdbc:postgresql://192.168.2.90:5432/swi_notifications\n",
      "Processing table: migrations\n",
      "Processing table: sys_tenants\n",
      "25/02/18 10:39:15 WARN HoodieBackedTableMetadata: Metadata table was not found at path s3a://datalake/bronze/notifications/Feb-18-2025/sys_tenants/.hoodie/metadata\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# WARNING: Unable to get Instrumentation. Dynamic Attach failed. You may add this JAR as -javaagent manually, or supply -Djdk.attach.allowAttachSelf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 27:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# WARNING: Unable to attach Serviceability Agent. Unable to attach even with module exceptions: [org.apache.hudi.org.openjdk.jol.vm.sa.SASupportException: Sense failed., org.apache.hudi.org.openjdk.jol.vm.sa.SASupportException: Sense failed., org.apache.hudi.org.openjdk.jol.vm.sa.SASupportException: Sense failed.]\n",
      "25/02/18 10:39:21 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-hbase.properties,hadoop-metrics2.properties\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished processing: sys_tenants\n",
      "Processing table: sys_organizations\n",
      "25/02/18 10:39:22 WARN HoodieBackedTableMetadata: Metadata table was not found at path s3a://datalake/bronze/notifications/Feb-18-2025/sys_organizations/.hoodie/metadata\n",
      "Finished processing: sys_organizations\n",
      "Processing table: sys_code_masters\n",
      "25/02/18 10:39:26 WARN HoodieBackedTableMetadata: Metadata table was not found at path s3a://datalake/bronze/notifications/Feb-18-2025/sys_code_masters/.hoodie/metadata\n",
      "Finished processing: sys_code_masters\n",
      "Processing table: zns_business_authentication\n",
      "25/02/18 10:39:28 WARN HoodieBackedTableMetadata: Metadata table was not found at path s3a://datalake/bronze/notifications/Feb-18-2025/zns_business_authentication/.hoodie/metadata\n",
      "Finished processing: zns_business_authentication\n",
      "Processing table: fcm_authentication\n",
      "25/02/18 10:39:33 WARN HoodieBackedTableMetadata: Metadata table was not found at path s3a://datalake/bronze/notifications/Feb-18-2025/fcm_authentication/.hoodie/metadata\n",
      "Finished processing: fcm_authentication\n",
      "Processing table: fcm_notification\n",
      "25/02/18 10:39:36 WARN HoodieBackedTableMetadata: Metadata table was not found at path s3a://datalake/bronze/notifications/Feb-18-2025/fcm_notification/.hoodie/metadata\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished processing: fcm_notification\n",
      "Processing table: zns_business_notification\n",
      "25/02/18 10:39:42 WARN HoodieBackedTableMetadata: Metadata table was not found at path s3a://datalake/bronze/notifications/Feb-18-2025/zns_business_notification/.hoodie/metadata\n",
      "Finished processing: zns_business_notification\n",
      "Processing table: vietguys_authentication\n",
      "25/02/18 10:39:46 WARN HoodieBackedTableMetadata: Metadata table was not found at path s3a://datalake/bronze/notifications/Feb-18-2025/vietguys_authentication/.hoodie/metadata\n",
      "Finished processing: vietguys_authentication\n",
      "Processing table: vietguys_notification\n",
      "25/02/18 10:39:48 WARN HoodieBackedTableMetadata: Metadata table was not found at path s3a://datalake/bronze/notifications/Feb-18-2025/vietguys_notification/.hoodie/metadata\n",
      "Finished processing: vietguys_notification\n",
      "Processing table: zalo_oa_authentication\n",
      "25/02/18 10:39:50 WARN HoodieBackedTableMetadata: Metadata table was not found at path s3a://datalake/bronze/notifications/Feb-18-2025/zalo_oa_authentication/.hoodie/metadata\n",
      "Finished processing: zalo_oa_authentication\n",
      "Processing table: zalo_oa_notification\n",
      "25/02/18 10:39:53 WARN HoodieBackedTableMetadata: Metadata table was not found at path s3a://datalake/bronze/notifications/Feb-18-2025/zalo_oa_notification/.hoodie/metadata\n",
      "Finished processing: zalo_oa_notification\n",
      "Processing table: zalo_oa_configuration\n",
      "25/02/18 10:39:56 WARN HoodieBackedTableMetadata: Metadata table was not found at path s3a://datalake/bronze/notifications/Feb-18-2025/zalo_oa_configuration/.hoodie/metadata\n",
      "Finished processing: zalo_oa_configuration\n"
     ]
    }
   ],
   "source": [
    "POSTGRES_USER='postgres'\n",
    "POSTGRES_PASSWORD='p0stgr3s'\n",
    "POSTGRES_DB='swi_notifications'\n",
    "POSTGRES_HOST='192.168.2.90'\n",
    "POSTGRES_PORT='5432'\n",
    "# PostgreSQL connection string\n",
    "postgres_url = f\"jdbc:postgresql://{POSTGRES_HOST}:{POSTGRES_PORT}/{POSTGRES_DB}\"\n",
    "\n",
    "# Fetch tables dynamically from PostgreSQL public schema\n",
    "def get_public_tables():\n",
    "    conn = psycopg2.connect(\n",
    "        dbname=POSTGRES_DB,\n",
    "        user=POSTGRES_USER,\n",
    "        password=POSTGRES_PASSWORD,\n",
    "        host=POSTGRES_HOST,\n",
    "        port=5432\n",
    "    )\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(\"SELECT table_name FROM information_schema.tables WHERE table_schema = 'public';\")\n",
    "    tables = [row[0] for row in cur.fetchall()]\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "    return tables\n",
    "\n",
    "# Get all tables from the public schema\n",
    "tables_names = get_public_tables()\n",
    "\n",
    "print(postgres_url)\n",
    "# Process each table\n",
    "for table_name in tables_names:\n",
    "    print(f\"Processing table: {table_name}\")\n",
    "    if table_name == 'migrations': continue\n",
    "    # Read data from PostgreSQL\n",
    "    df = spark.read \\\n",
    "        .format(\"jdbc\") \\\n",
    "        .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "        .option(\"url\", postgres_url) \\\n",
    "        .option(\"dbtable\", f\"public.{table_name}\") \\\n",
    "        .option(\"user\", POSTGRES_USER) \\\n",
    "        .option(\"password\", POSTGRES_PASSWORD) \\\n",
    "        .load()\n",
    "\n",
    "    df.write \\\n",
    "        .format(\"hudi\") \\\n",
    "        .option(\"hoodie.table.name\", table_name) \\\n",
    "        .option(\"hoodie.datasource.write.recordkey.field\", \"id\") \\\n",
    "        .option(\"hoodie.datasource.write.partitionpath.field\", \"updated_at\") \\\n",
    "        .option(\"hoodie.datasource.write.precombine.field\", \"created_at\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .save(f\"s3a://datalake/bronze/notifications/{today}/{table_name}\")\n",
    "\n",
    "    print(f\"Finished processing: {table_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07c59684-c3c7-4f4b-b742-83f6afa0c496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/spark/jars/postgresql-42.3.5.jar\n"
     ]
    }
   ],
   "source": [
    "!ls /opt/spark/jars/postgresql-*.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e890e9cd-7807-43a6-878b-da5dba299e4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
